<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>WebGPU Life</title>
  </head>
  <body>
    <canvas width="512" height="512"></canvas>
    <script type="module">
      const canvas = document.querySelector("canvas");
      if (!navigator.gpu) {
        throw new Error("WebGPU not supported on this browser.");
      }
      const adapter = await navigator.gpu.requestAdapter();
      if (!adapter) {
        throw new Error("No appropriate GPUAdapter found.");
      }
      const device = await adapter.requestDevice();
      if (!device) {
        throw new Error("No appropriate GPUDevice found.");
      }
      const context = canvas.getContext("webgpu");
      if (!context) {
        throw new Error("WebGPU not supported on this canvas.");
      }
      const canvasFormat = navigator.gpu.getPreferredCanvasFormat();
      context.configure({
        device,
        format: canvasFormat,
      });
      const vertices = new Float32Array([
        // Triangle 1 (Blue)
        -0.8, -0.8,
        //   X,    Y,
        0.8, -0.8,
        //   X,    Y,
        0.8, 0.8,
        // Triangle 2 (Red)
        -0.8, -0.8,
        //   X,    Y,
        0.8, 0.8,
        //   X,    Y,
        -0.8, 0.8,
      ]);
      const vertexBuffer = device.createBuffer({
        label: "Cell vertices",
        size: vertices.byteLength,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(vertexBuffer, /*bufferOffset=*/ 0, vertices);
      const vertexBufferLayout = {
        arrayStride: 8,
        attributes: [
          {
            format: "float32x2",
            offset: 0,
            shaderLocation: 0, // Position, see vertex shader
          },
        ],
      };
      /* Here in the shared  vec4f(pos,...) is possible since pos is a vec2f and contains x and y values.
       * The alternative is to use vec2f(pos.x, pos.y, 0, 1) */
      const cellShaderModule = device.createShaderModule({
        label: "Cell shader",
        code: `
    @vertex
    fn vertexMain(@location(0) pos: vec2f) ->
          @builtin(position) vec4f {
      return vec4f(pos, 0, 1);
    }

    @fragment
    fn fragmentMain() -> @location(0) vec4f {
      return vec4f(1, 0, 0, 1); // (Red, Green, Blue, Alpha)
    }
  `,
      });
      const cellPipeline = device.createRenderPipeline({
        label: "Cell pipeline",
        layout: "auto",
        vertex: {
          module: cellShaderModule,
          entryPoint: "vertexMain",
          buffers: [vertexBufferLayout],
        },
        fragment: {
          module: cellShaderModule,
          entryPoint: "fragmentMain",
          targets: [
            {
              format: canvasFormat,
            },
          ],
        },
      });
      const encoder = device.createCommandEncoder();
      if (!encoder) {
        throw new Error("No appropriate GPUCommandEncoder found.");
      }
      const pass = encoder.beginRenderPass({
        colorAttachments: [
          {
            view: context.getCurrentTexture().createView(),
            loadOp: "clear",
            clearValue: { r: 0, g: 0, b: 0.4, a: 1 }, // New line
            storeOp: "store",
          },
        ],
      });
      pass.setPipeline(cellPipeline);
      pass.setVertexBuffer(0, vertexBuffer);
      pass.draw(vertices.length / 2); // 6 vertices
      pass.end();
      const commandBuffer = encoder.finish();
      device.queue.submit([commandBuffer]);
    </script>
  </body>
</html>
